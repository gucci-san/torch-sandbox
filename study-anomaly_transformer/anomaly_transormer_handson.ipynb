{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly-Transformerとりあえず動いたので、何してるか精査\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "\n",
    "def mkdir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os, argparse\n",
    "from torch.backends import cudnn\n",
    "\n",
    "config = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"k\": 3,\n",
    "    \"win_size\":100,\n",
    "    \"pretrained_model\": None,\n",
    "    \"anomaly_ratio\": 1,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 256,\n",
    "    \"mode\": \"train\",\n",
    "    \"dataset\": \"SMAP\",\n",
    "    \"data_path\": \"./dataset/SMAP\",\n",
    "    \"input_c\": 25,\n",
    "    \"output_c\": 25,\n",
    "    \"model_save_path\": \"checkpoints\",\n",
    "    \"anormly_ratio\": 1,\n",
    "}\n",
    "\n",
    "# def main(config):\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの解読"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: (427617, 25)\n",
      "train: (135183, 25)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import collections\n",
    "import numbers\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "class SMAPSegLoader(object):\n",
    "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        self.win_size = win_size\n",
    "        self.scaler = StandardScaler()\n",
    "        data = np.load(data_path + \"/SMAP_train.npy\")\n",
    "        self.scaler.fit(data)\n",
    "        data = self.scaler.transform(data)\n",
    "        test_data = np.load(data_path + \"/SMAP_test.npy\") # これ、max(train) < max(test)だとバグらね？ --\n",
    "        self.test = self.scaler.transform(test_data)\n",
    "\n",
    "        self.train = data\n",
    "        self.val = self.test\n",
    "        self.test_labels = np.load(data_path + \"/SMAP_test_label.npy\")\n",
    "        print(\"test:\", self.test.shape)\n",
    "        print(\"train:\", self.train.shape) # この時点ではnp.load()と同じshapeを持っている --\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
    "        elif (self.mode == 'val'):\n",
    "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
    "        elif (self.mode == 'test'):\n",
    "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
    "        else:\n",
    "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index * self.step\n",
    "        if self.mode == \"train\":\n",
    "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
    "        elif (self.mode == 'val'):\n",
    "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
    "        elif (self.mode == 'test'):\n",
    "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
    "                self.test_labels[index:index + self.win_size])\n",
    "        else:\n",
    "            return np.float32(self.test[\n",
    "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
    "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n",
    "\n",
    "\n",
    "def get_loader_segment(data_path, batch_size, win_size=100, step=100, mode='train', dataset='KDD'):\n",
    "    if (dataset == 'SMAP'):\n",
    "        dataset = SMAPSegLoader(data_path, win_size, 1, mode)\n",
    "\n",
    "    shuffle = False\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=0)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model, solverクラス --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kl_loss(p, q):\n",
    "    res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
    "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr_):\n",
    "    lr_adjust = {epoch: lr_ * (0.5 ** ((epoch - 1) // 1))}\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, dataset_name='', delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_score2 = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_loss2_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.dataset = dataset_name\n",
    "\n",
    "    def __call__(self, val_loss, val_loss2, model, path):\n",
    "        score = -val_loss\n",
    "        score2 = -val_loss2\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_score2 = score2\n",
    "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
    "        elif score < self.best_score + self.delta or score2 < self.best_score2 + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_score2 = score2\n",
    "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_loss2, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), os.path.join(path, str(self.dataset) + '_checkpoint.pth'))\n",
    "        self.val_loss_min = val_loss\n",
    "        self.val_loss2_min = val_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class AnomalyAttention(nn.Module):\n",
    "    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        window_size = win_size\n",
    "        self.distances = torch.zeros((window_size, window_size)).cuda()\n",
    "        for i in range(window_size):\n",
    "            for j in range(window_size):\n",
    "                self.distances[i][j] = abs(i - j)\n",
    "\n",
    "    def forward(self, queries, keys, values, sigma, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "        attn = scale * scores\n",
    "\n",
    "        sigma = sigma.transpose(1, 2)  # B L H ->  B H L\n",
    "        window_size = attn.shape[-1]\n",
    "        sigma = torch.sigmoid(sigma * 5) + 1e-5\n",
    "        sigma = torch.pow(3, sigma) - 1\n",
    "        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size)  # B H L L\n",
    "        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1).cuda()\n",
    "        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))\n",
    "\n",
    "        series = self.dropout(torch.softmax(attn, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", series, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), series, prior, sigma)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model,\n",
    "                                          d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model,\n",
    "                                        d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model,\n",
    "                                          d_values * n_heads)\n",
    "        self.sigma_projection = nn.Linear(d_model,\n",
    "                                          n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "        x = queries\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "        sigma = self.sigma_projection(x).view(B, L, H)\n",
    "\n",
    "        out, series, prior, sigma = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            sigma,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), series, prior, sigma\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.0):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn, mask, sigma = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn, mask, sigma\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        series_list = []\n",
    "        prior_list = []\n",
    "        sigma_list = []\n",
    "        for attn_layer in self.attn_layers:\n",
    "            x, series, prior, sigma = attn_layer(x, attn_mask=attn_mask)\n",
    "            series_list.append(series)\n",
    "            prior_list.append(prior)\n",
    "            sigma_list.append(sigma)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, series_list, prior_list, sigma_list\n",
    "\n",
    "\n",
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, win_size, enc_in, c_out, d_model=512, n_heads=8, e_layers=3, d_ff=512,\n",
    "                 dropout=0.0, activation='gelu', output_attention=True):\n",
    "        super(AnomalyTransformer, self).__init__()\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Encoding\n",
    "        self.embedding = DataEmbedding(enc_in, d_model, dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        AnomalyAttention(win_size, False, attention_dropout=dropout, output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out = self.embedding(x)\n",
    "        enc_out, series, prior, sigmas = self.encoder(enc_out)\n",
    "        enc_out = self.projection(enc_out)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return enc_out, series, prior, sigmas\n",
    "        else:\n",
    "            return enc_out  # [B, L, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: (427617, 25)\n",
      "train: (135183, 25)\n",
      "test: (427617, 25)\n",
      "train: (135183, 25)\n",
      "test: (427617, 25)\n",
      "train: (135183, 25)\n",
      "test: (427617, 25)\n",
      "train: (135183, 25)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Solver(object):\n",
    "    DEFAULTS = {}\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.__dict__.update(Solver.DEFAULTS, **config)\n",
    "\n",
    "        self.train_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                               mode='train',\n",
    "                                               dataset=self.dataset)\n",
    "        self.vali_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                              mode='val',\n",
    "                                              dataset=self.dataset)\n",
    "        self.test_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                              mode='test',\n",
    "                                              dataset=self.dataset)\n",
    "        self.thre_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                              mode='thre',\n",
    "                                              dataset=self.dataset)\n",
    "\n",
    "        self.build_model()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = AnomalyTransformer(win_size=self.win_size, enc_in=self.input_c, c_out=self.output_c, e_layers=3)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def vali(self, vali_loader):\n",
    "        self.model.eval()\n",
    "\n",
    "        loss_1 = []\n",
    "        loss_2 = []\n",
    "        for i, (input_data, _) in enumerate(vali_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                series_loss += (torch.mean(my_kl_loss(series[u], (\n",
    "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                               self.win_size)).detach())) + torch.mean(\n",
    "                    my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)).detach(),\n",
    "                        series[u])))\n",
    "                prior_loss += (torch.mean(\n",
    "                    my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       self.win_size)),\n",
    "                               series[u].detach())) + torch.mean(\n",
    "                    my_kl_loss(series[u].detach(),\n",
    "                               (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       self.win_size)))))\n",
    "            series_loss = series_loss / len(prior)\n",
    "            prior_loss = prior_loss / len(prior)\n",
    "\n",
    "            rec_loss = self.criterion(output, input)\n",
    "            loss_1.append((rec_loss - self.k * series_loss).item())\n",
    "            loss_2.append((rec_loss + self.k * prior_loss).item())\n",
    "\n",
    "        return np.average(loss_1), np.average(loss_2)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        print(\"======================TRAIN MODE======================\")\n",
    "\n",
    "        time_now = time.time()\n",
    "        path = self.model_save_path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        early_stopping = EarlyStopping(patience=3, verbose=True, dataset_name=self.dataset)\n",
    "        train_steps = len(self.train_loader)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            iter_count = 0\n",
    "            loss1_list = []\n",
    "\n",
    "            epoch_time = time.time()\n",
    "            self.model.train()\n",
    "            for i, (input_data, labels) in enumerate(self.train_loader):\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                iter_count += 1\n",
    "                input = input_data.float().to(self.device)\n",
    "\n",
    "                output, series, prior, _ = self.model(input)\n",
    "\n",
    "                # calculate Association discrepancy\n",
    "                series_loss = 0.0\n",
    "                prior_loss = 0.0\n",
    "                for u in range(len(prior)):\n",
    "                    series_loss += (torch.mean(my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach())) + torch.mean(\n",
    "                        my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                           self.win_size)).detach(),\n",
    "                                   series[u])))\n",
    "                    prior_loss += (torch.mean(my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach())) + torch.mean(\n",
    "                        my_kl_loss(series[u].detach(), (\n",
    "                                prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       self.win_size)))))\n",
    "                series_loss = series_loss / len(prior)\n",
    "                prior_loss = prior_loss / len(prior)\n",
    "\n",
    "                rec_loss = self.criterion(output, input)\n",
    "\n",
    "                loss1_list.append((rec_loss - self.k * series_loss).item())\n",
    "                loss1 = rec_loss - self.k * series_loss\n",
    "                loss2 = rec_loss + self.k * prior_loss\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.num_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                # Minimax strategy\n",
    "                loss1.backward(retain_graph=True)\n",
    "                loss2.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(loss1_list)\n",
    "\n",
    "            vali_loss1, vali_loss2 = self.vali(self.test_loader)\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} \".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss1))\n",
    "            early_stopping(vali_loss1, vali_loss2, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(self.optimizer, epoch + 1, self.lr)\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(str(self.model_save_path), str(self.dataset) + '_checkpoint.pth')))\n",
    "        self.model.eval()\n",
    "        temperature = 50\n",
    "\n",
    "        print(\"======================TEST MODE======================\")\n",
    "\n",
    "        criterion = nn.MSELoss(reduce=False)\n",
    "\n",
    "        # (1) stastic on the train set\n",
    "        attens_energy = []\n",
    "        for i, (input_data, labels) in enumerate(self.train_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "            loss = torch.mean(criterion(input, output), dim=-1)\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                if u == 0:\n",
    "                    series_loss = my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss = my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "                else:\n",
    "                    series_loss += my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss += my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "\n",
    "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "            cri = metric * loss\n",
    "            cri = cri.detach().cpu().numpy()\n",
    "            attens_energy.append(cri)\n",
    "\n",
    "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
    "        train_energy = np.array(attens_energy)\n",
    "\n",
    "        # (2) find the threshold\n",
    "        attens_energy = []\n",
    "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "\n",
    "            loss = torch.mean(criterion(input, output), dim=-1)\n",
    "\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                if u == 0:\n",
    "                    series_loss = my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss = my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "                else:\n",
    "                    series_loss += my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss += my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "            # Metric\n",
    "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "            cri = metric * loss\n",
    "            cri = cri.detach().cpu().numpy()\n",
    "            attens_energy.append(cri)\n",
    "\n",
    "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
    "        test_energy = np.array(attens_energy)\n",
    "        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n",
    "        thresh = np.percentile(combined_energy, 100 - self.anormly_ratio)\n",
    "        print(\"Threshold :\", thresh)\n",
    "\n",
    "        # (3) evaluation on the test set\n",
    "        test_labels = []\n",
    "        attens_energy = []\n",
    "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "\n",
    "            loss = torch.mean(criterion(input, output), dim=-1)\n",
    "\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                if u == 0:\n",
    "                    series_loss = my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss = my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "                else:\n",
    "                    series_loss += my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss += my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "\n",
    "            cri = metric * loss\n",
    "            cri = cri.detach().cpu().numpy()\n",
    "            attens_energy.append(cri)\n",
    "            test_labels.append(labels)\n",
    "\n",
    "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
    "        test_labels = np.concatenate(test_labels, axis=0).reshape(-1)\n",
    "        test_energy = np.array(attens_energy)\n",
    "        test_labels = np.array(test_labels)\n",
    "\n",
    "        pred = (test_energy > thresh).astype(int)\n",
    "\n",
    "        gt = test_labels.astype(int)\n",
    "\n",
    "        print(\"pred:   \", pred.shape)\n",
    "        print(\"gt:     \", gt.shape)\n",
    "\n",
    "        # detection adjustment\n",
    "        anomaly_state = False\n",
    "        for i in range(len(gt)):\n",
    "            if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
    "                anomaly_state = True\n",
    "                for j in range(i, 0, -1):\n",
    "                    if gt[j] == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        if pred[j] == 0:\n",
    "                            pred[j] = 1\n",
    "                for j in range(i, len(gt)):\n",
    "                    if gt[j] == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        if pred[j] == 0:\n",
    "                            pred[j] = 1\n",
    "            elif gt[i] == 0:\n",
    "                anomaly_state = False\n",
    "            if anomaly_state:\n",
    "                pred[i] = 1\n",
    "\n",
    "        pred = np.array(pred)\n",
    "        gt = np.array(gt)\n",
    "        print(\"pred: \", pred.shape)\n",
    "        print(\"gt:   \", gt.shape)\n",
    "\n",
    "        from sklearn.metrics import precision_recall_fscore_support\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(gt, pred)\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(gt, pred,\n",
    "                                                                              average='binary')\n",
    "        print(\n",
    "            \"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} \".format(\n",
    "                accuracy, precision,\n",
    "                recall, f_score))\n",
    "\n",
    "        return accuracy, precision, recall, f_score\n",
    "\n",
    "solver = Solver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================TRAIN MODE======================\n",
      "\tspeed: 0.2075s/iter; left time: 308.1052s\n",
      "\tspeed: 0.1966s/iter; left time: 272.3286s\n",
      "\tspeed: 0.1963s/iter; left time: 252.1842s\n",
      "\tspeed: 0.1969s/iter; left time: 233.3848s\n",
      "\tspeed: 0.1981s/iter; left time: 214.9525s\n",
      "Epoch: 1 cost time: 105.11766219139099\n",
      "Epoch: 1, Steps: 528 | Train Loss: -44.5152232 Vali Loss: -46.9635691 \n",
      "Validation loss decreased (inf --> -46.963569).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\tspeed: 1.2794s/iter; left time: 1224.4121s\n",
      "\tspeed: 0.1980s/iter; left time: 169.6814s\n",
      "\tspeed: 0.1992s/iter; left time: 150.7622s\n",
      "\tspeed: 0.1975s/iter; left time: 129.7367s\n",
      "\tspeed: 0.1974s/iter; left time: 109.9440s\n",
      "Epoch: 2 cost time: 104.40921354293823\n",
      "Epoch: 2, Steps: 528 | Train Loss: -47.8671635 Vali Loss: -47.8101966 \n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\tspeed: 1.2868s/iter; left time: 552.0217s\n",
      "\tspeed: 0.2000s/iter; left time: 65.8046s\n",
      "\tspeed: 0.2000s/iter; left time: 45.8026s\n",
      "\tspeed: 0.2000s/iter; left time: 25.8013s\n",
      "\tspeed: 0.2000s/iter; left time: 5.7996s\n",
      "Epoch: 3 cost time: 105.47253346443176\n",
      "Epoch: 3, Steps: 528 | Train Loss: -48.1957150 Vali Loss: -47.9437112 \n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n"
     ]
    }
   ],
   "source": [
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================TEST MODE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marimo/poetry_projects/torch-sandbox/.venv/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold : 0.0016624154523015022\n",
      "pred:    (427600,)\n",
      "gt:      (427600,)\n",
      "pred:  (427600,)\n",
      "gt:    (427600,)\n",
      "Accuracy : 0.9821, Precision : 0.9369, Recall : 0.9222, F-score : 0.9295 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9821000935453695,\n",
       " 0.9369171898800015,\n",
       " 0.9221515284481497,\n",
       " 0.9294757209988022)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全に動いた"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42ee1f43f6c0981f6814693771d67f1a3a6ce9120aa5e8c9a465fdc04f30401e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
